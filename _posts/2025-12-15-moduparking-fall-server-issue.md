---
layout: post
title: "[모두의주차장] 매년 가을 서버가 죽는 날"
subtitle: 불꽃축제 트래픽 15배 개선기
date: 2025-12-15 00:00:00 +0900
category: backend
background: "/assets/images/default-bg.jpg"
author: tonic
comments: true
tags:
  - backend
  - performance
  - redis
  - cache
---

# 목차

1. [개요](#1-개요)
2. [문제 상황](#2-문제-상황)
   1. [3년 연속 서버 장애](#21-3년-연속-서버-장애)
   2. [장애의 근본 원인](#22-장애의-근본-원인)
3. [해결 방안](#3-해결-방안)
   1. [API 구조 개선: 조회와 갱신의 분리](#31-api-구조-개선-조회와-갱신의-분리)
   2. [캐시 전략 도입](#32-캐시-전략-도입)
   3. [지오해시 기반 캐시 설계](#33-지오해시-기반-캐시-설계)
   4. [CDC를 활용한 비동기 캐시 갱신](#34-cdc를-활용한-비동기-캐시-갱신)
4. [결과](#4-결과)
5. [마무리](#5-마무리)

---

# 1. 개요

모두의주차장은 쏘카에서 운영하는 주차장 공유 플랫폼입니다. 저렴하게 주차를 이용할 수 있으며, 현재 당일 판매가 가능하고 예매 기능도 준비 중입니다.

모두의주차장에는 매년 하루 서버가 죽는 날이 있습니다. 바로 **불꽃축제** 날입니다.

매년 10월경 서울에서 진행되는 불꽃축제 날 자정, 모두의주차장 서버는 3년 연속 장애를 겪어왔습니다. 올해는 근본적인 해결책을 마련하고자 성능 개선 프로젝트를 진행했고, 그 과정을 공유합니다.

# 2. 문제 상황

## 2.1 3년 연속 서버 장애

불꽃축제 대응 히스토리는 다음과 같습니다:
- **3년 전**: 스케일아웃 1.5배 → 장애 발생
- **2년 전**: 스케일아웃 2배 → 장애 발생
- **1년 전**: 스케일아웃 2.5배 → 장애 발생 + DB 장애까지 발생

매년 스케일아웃 배수를 늘렸지만 장애는 반복됐습니다. 서비스가 연 30% 이상 성장하면서 매번 예측치를 초과하는 트래픽이 몰렸기 때문입니다.

그렇다면 올해는 3배를 늘리면 됐을까요? 아닙니다. 저는 근본적인 문제를 해결하고자 프로젝트를 발의했습니다.

## 2.2 장애의 근본 원인

모두의주차장에서 가장 트래픽이 높은 기능은 **지도 기반 주차장/주차권 조회**입니다.

모두의주차장은 10년 이상 모놀리식 아키텍처로 운영되다가 점진적으로 MSA로 전환되었습니다. 지도 핀(주차장 위치 표시) 조회를 전담하는 조회 전용 서버가 분리되어 별도 아키텍처로 운영 중이었습니다.

> 관련 발표 영상: [YouTube 링크](https://www.youtube.com/watch?v=47x1Z4khpB0&list=PLEZRFe2YmFou3R6vTrfFIXz-pusnBnxB8&index=8)

그럼에도 서버가 죽었던 이유는 무엇일까요?

### 조회 API에서 데이터를 갱신하는 구조적 문제

지도에서 주차장 정보를 조회하면 각 주차장에 해당하는 주차상품 리스트도 함께 조회해야 합니다. 문제는 이 **조회 API가 단순 조회가 아니었다**는 점입니다.

모두의주차장은 **당일 판매**만 가능합니다. 자정이 되면 모든 주차권의 매진 여부가 초기화되어야 하는데, 기존 구조에서는 **조회 시점에 매진 여부를 갱신(DB 업데이트)**하고 있었습니다.

<div class="mermaid">
flowchart LR
    A[조회 요청] --> B[주차장 조회]
    B --> C[주차상품 조회]
    C --> D[매진여부 갱신]
    D --> E[DB Update]
    E --> F[응답]

    style D fill:#ff6b6b
    style E fill:#ff6b6b
</div>

즉, **조회(Query)인데 내부에서 갱신(Command)을 수행**하는 구조였습니다. 트래픽이 적을 때는 문제가 없었지만, 불꽃축제처럼 대량의 조회 요청이 몰리면 갱신 요청도 함께 폭증했습니다.

### 자정의 연쇄 장애

불꽃축제 날 자정, 수많은 사용자가 동시에 주차권을 조회하면서 다음과 같은 연쇄 장애가 발생했습니다:

1. 대량의 매진 여부 갱신 요청이 **Bull Queue(Redis 기반)**에 적재
2. Redis 메모리 용량 초과
3. API 응답 지연 → DB 직접 조회 폭증
4. DB 병목 → 판매 상태 변경 배치까지 영향

<div class="mermaid">
flowchart TD
    A[자정: 대량 조회 요청] --> B[매진여부 갱신 폭증]
    B --> C[Bull Queue 적재]
    C --> D[Redis 용량 초과]
    D --> E[API 응답 지연]
    E --> F[DB 직접 조회 폭증]
    F --> G[DB 병목 발생]
    G --> H[전체 서비스 장애]

    style D fill:#ff6b6b
    style G fill:#ff6b6b
    style H fill:#ff6b6b
</div>

단순히 스케일아웃으로는 해결할 수 없는, **API 구조 자체의 문제**였습니다.

# 3. 해결 방안

## 3.1 API 구조 개선: 조회와 갱신의 분리

핵심은 **조회 API에서 데이터 갱신을 제거**하는 것이었습니다.

기존에는 매진 여부를 DB에 저장된 상태값으로 관리했기 때문에, 자정마다 이 값을 갱신해야 했습니다. 이 구조를 다음과 같이 변경했습니다:

**기존 구조:**
- 조회 시 매진 여부 상태값을 DB에서 읽음
- 자정에 상태값 초기화를 위해 DB 업데이트 필요

**개선된 구조:**
- 조회 시 **판매량 캐시**를 읽어 매진 여부를 계산
- 자정이 되면 날짜가 바뀌므로 자동으로 새로운 캐시 키 참조
- **조회는 순수하게 조회만**, 갱신은 별도 비동기 파이프라인에서 처리

<div class="mermaid">
flowchart LR
    subgraph 기존
        A1[조회 요청] --> B1[DB 조회 + 갱신]
    end

    subgraph 개선
        A2[조회 요청] --> B2[캐시 조회만]
        C2[판매 발생] --> D2[CDC] --> E2[캐시 갱신]
    end

    style B1 fill:#ff6b6b
    style B2 fill:#4ecdc4
    style E2 fill:#4ecdc4
</div>

이 구조 변경으로 조회와 갱신의 책임이 분리되었습니다. 조회 트래픽이 폭증해도 트랜잭션이나 데이터 갱신에 영향을 주지 않고, 각각 독립적으로 확장할 수 있는 구조가 되었습니다.

## 3.2 캐시 전략 도입

조회와 갱신을 분리하려면 캐시가 필수였습니다. Redis를 도입하고, 여러 캐시 전략을 조합하여 적용했습니다.

- **Look-Aside(Cache-Aside)**: 조회 시 캐시를 우선 확인하고, 미스 시 DB 조회 후 캐시에 저장
- **CDC 기반 비동기 갱신**: 데이터 변경 시 CDC 파이프라인을 통해 캐시를 비동기로 갱신
- **날짜 기반 캐시 키**: 날짜가 키에 포함되어 자정에 자동으로 새 캐시 참조 (별도 초기화 불필요)

> 캐시 전략 관련 참고: [인증 토큰 트래픽 처리기 - 올리](https://tech.socarcorp.kr/dev/2023/06/27/handling-authentication-token-traffic-01.html#%EB%AC%B8%EC%A0%9C-%ED%95%B4%EA%B2%B0-%EB%B0%A9%EC%95%88-%EC%A0%81%EC%9A%A9---redis-cache-layer-%EC%BA%90%EC%8B%9C-%EC%A0%84%EB%9E%B5)

순간 트래픽 대응이 목적이었기에 TTL을 짧게 설정했고, 무효화 API도 마련해두었습니다.

## 3.3 지오해시 기반 캐시 설계

유저마다 지도를 조회하는 범위가 각자 다르고, 다양하게 움직이며 조회합니다. 캐시 효율을 높이기 위한 전략으로 **지오해시(Geohash)**를 선택했습니다.

<div class="mermaid">
flowchart TB
    A[클라이언트 지도 조회] --> B[Geohash 변환]
    B --> C[Redis: 지오해시 → 주차장 IDs]
    C --> D[Redis: 주차장 상세정보 MGET]
    D --> E[Redis: 주차권 리스트]
    E --> F[Redis: 날짜별 판매량]
    F --> G[매진여부 계산 후 응답]

    C -.->|Cache Miss| H[DB 조회 후 캐시 저장]
    D -.->|Cache Miss| H
    E -.->|Cache Miss| H

    style C fill:#4ecdc4
    style D fill:#4ecdc4
    style E fill:#4ecdc4
    style F fill:#4ecdc4
</div>

캐시 구조는 다음과 같습니다:
1. 지오해시를 기반으로 주차장 고유 번호를 Set으로 저장
2. 해당 결과에서 각 주차장의 상세 정보를 `MGET`으로 조회
3. 주차장에 해당하는 주차권 리스트를 캐시에서 조회
4. **날짜별 판매량 캐시**를 읽어 매진 여부를 계산

### 캐시 키 설계

```
# 1. 지오해시 → 주차장 ID 목록 (SET)
geo:parkinglot:{geohash}
  → SET { "parkinglot:1001", "parkinglot:1002", "parkinglot:1003" }

# 2. 주차장 상세 정보 (STRING)
parkinglot:detail:{parkinglot_id}
  → { "id": 1001, "name": "강남역 주차장", "address": "...", ... }

# 3. 주차장별 주차권 리스트 (STRING)
parkinglot:tickets:{parkinglot_id}
  → [ { "id": 5001, "price": 5000, ... }, { "id": 5002, ... } ]

# 4. 날짜별 판매량 (STRING) - 핵심!
ticket:sales:{ticket_id}:{date}
  → { "sold": 45, "total": 50 }
```

**날짜별 판매량 캐시**가 핵심입니다. 자정이 되면 날짜가 바뀌므로, 새로운 캐시 키(`ticket:sales:5001:2024-10-06`)를 참조하게 됩니다. 별도의 초기화 작업 없이 자연스럽게 매진 여부가 리셋됩니다.

모든 캐시에 히트하면 DB를 거치지 않고 빠르게 응답할 수 있습니다.

| 캐시 종류 | TTL | 갱신 방식 |
|----------|-----|----------|
| 지오해시 → 주차장 ID | 중간 | 주차장 등록/삭제 시 |
| 주차장 상세 정보 | 중간 | 정보 변경 시 |
| 주차권 리스트 | 짧음 | 주차권 변경 시 |
| 날짜별 판매량 | 매우 짧음 | CDC 실시간 갱신 |

DB를 거치지 않더라도 Redis를 최대 3회 조회해야 하므로 네트워크 비용이 발생할 수 있습니다. 하지만 트레이드오프로서 캐시 관리 안정성을 택했고, `MGET`을 적극 활용하여 효율을 높였습니다.

## 3.4 CDC를 활용한 비동기 캐시 갱신

조회 API에서 갱신을 제거했으니, 캐시는 어떻게 최신 상태를 유지할까요? **CDC(Change Data Capture)** 기반의 비동기 파이프라인을 구축했습니다.

> **CDC(Change Data Capture)란?**
> 서비스 로직과 별개로 DB 변경 사항을 감지하여 Kafka로 스트리밍하는 기술입니다.

기존에 AWS DMS를 통해 Kafka로 메시지 스트리밍되던 파이프라인이 있었습니다. 토픽이 잘 설계되어 있었기에 캐시 갱신 파이프라인을 쉽게 구축할 수 있었습니다.

<div class="mermaid">
flowchart LR
    subgraph 판매 서비스
        A[판매 API] --> B[(Database)]
    end

    subgraph CDC 파이프라인
        B --> C[AWS DMS]
        C --> D[Kafka]
    end

    subgraph 캐시 갱신
        D --> E[Consumer]
        E --> F[(Redis Cache)]
    end

    subgraph 조회 서비스
        G[지도 API] --> F
    end

    style F fill:#4ecdc4
</div>

이 구조의 장점:
- **판매가 발생하면** → CDC가 감지 → 판매량 캐시 자동 갱신
- **조회 API는 캐시만 읽음** → DB 부하 없음
- **자정 초기화 불필요** → 날짜 기반 캐시 키로 자동 해결

# 4. 결과

기존 성능 지표:
- **100 TPS**: 레이턴시가 밀리기 시작하는 기준
- **300 TPS**: 서버가 죽는 기준
- 불꽃축제 시 **600 TPS 이상**의 트래픽 발생

모두의주차장은 매년 30% 이상의 유저 성장을 이뤘기에 **1,000 TPS**를 목표로 설정했습니다.

개선 결과:
- 목표였던 1,000 TPS의 1.5배인 **1,500 TPS 이상** 달성
- 평균 레이턴시 **P95/P99 절반 이하**로 단축
- 기존 대비 **약 15배 성능 개선**

실제 불꽃축제 당일, 약 700 TPS의 트래픽이 몰렸고 서버는 안정적으로 버텨냈습니다. 그 결과 **모두의주차장 일 거래액 최대치**를 달성하는 쾌거를 이루었습니다.

# 5. 마무리

이번 프로젝트의 핵심은 단순한 성능 튜닝이 아니라 **API 구조의 근본적인 개선**이었습니다.

- 조회 API에서 갱신 로직 제거
- 캐시 기반 읽기 전용 구조로 전환
- 갱신은 CDC 기반 비동기 파이프라인으로 분리

스케일아웃만으로는 해결할 수 없었던 문제를, 구조를 바꿈으로써 해결한 사례입니다.

해당 프로젝트를 위한 사전 작업을 꾸준히 진행해왔고, 설계와 코드 리뷰를 열심히 지원해주신 팀원들에게 감사드립니다.
